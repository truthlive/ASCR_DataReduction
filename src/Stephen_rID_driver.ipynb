{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing out the ID algoritihm of Bhaskara et al.\n",
    "Stephen Becker, March 6 2023\n",
    "\n",
    "\"Residual Based Sampling for Online Low Rank Approximation\", by Aditya Bhaskara, Silvio Lattanzi, Sergei Vassilvitskii and Morteza Zadimoghaddam. FOCS, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import h5py\n",
    "from numpy.random import default_rng\n",
    "import scipy.linalg as sli\n",
    "import scipy.io as sio\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import rID"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a Python \"class\" for stored columns for the ID.  You can add columns to the class (and their index), and the class will maintain a QR decomposition of the columns\n",
    "\n",
    "This particular class has three \"lengths\" associated with it:\n",
    "1. `k_max` is the maximum number of columns\n",
    "2. `k_current` is the current number of columns that have been added to it\n",
    "3. `k_previous` <= `k_current` is the number of columns in the \"previous\" set (using Bhaskara et al.'s notation). By default, projection is done with respect to the \"previous\" set\n",
    "    - Calling the method `.mergePreviousWithCurrent()` will include *all* the columns into the \"previous\" set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stored_columns:\n",
    "    def __init__(self, k = np.Inf):\n",
    "        self.indices = []   # indices of the columns; of size k_current\n",
    "        self.k_max   = k    # upper limit on # of columns\n",
    "        self.k_current  = 0\n",
    "        self.k_previous = 0 # <= k_current\n",
    "        self.columns = []   # the columns\n",
    "        self.Q = []         # orthonormal version of the columns\n",
    "    def mergePreviousWithCurrent(self):\n",
    "        self.k_previous = self.k_current\n",
    "\n",
    "    def addColumn(self, newCol, newInd):\n",
    "        \"\"\" adds a column (and its index) to the set\"\"\"\n",
    "        if self.k_current >= self.k_max:\n",
    "            # do nothing. Should we give a warning??\n",
    "            return\n",
    "        elif self.k_current == 0:\n",
    "            # No need to orthogonalize it since nothing yet in the set. Just normalize it\n",
    "            self.columns = np.reshape(newCol, (-1,1) ) # ensure it is column vector of size (n,1), not size (n,)\n",
    "            self.Q = np.reshape( newCol / sli.norm(newCol), (-1,1) )\n",
    "        else:\n",
    "            # Update QR: orthogonalize x <- x - QQ'x, and then normalize\n",
    "            q            = newCol - self.project(newCol, full=True)\n",
    "            self.Q       = np.column_stack( (self.Q, q/sli.norm(q)) )\n",
    "            self.columns = np.column_stack( (self.columns, newCol ) )\n",
    "        self.indices.append( newInd )\n",
    "        self.k_current += 1\n",
    "    \n",
    "    # Todo: allow adding multiple columns at once: do X - project, then do QR \n",
    "    # Todo: allow a re-orthogonalization routine to account for loss in precision\n",
    "    \n",
    "    def project(self, X, justQtX = False, full=False):\n",
    "        \"\"\" Projects X into the range of chosen columns \n",
    "        Implicitly restricts to just \"k_previous\"\n",
    "        This does Q*Q'*X, but if justQtX is True, then does Q'*X (saves some time if you just need the Euclidean norm)\n",
    "        \"\"\"\n",
    "        if full == False:\n",
    "            k = self.k_previous\n",
    "        else:\n",
    "            k = self.k_current\n",
    "        if k==0:\n",
    "            if justQtX:\n",
    "                return np.zeros( size=(1,X.shape[1]) ) # not sure what a good shape is here...\n",
    "            else:\n",
    "                return 0*X\n",
    "        QtX = self.Q[:,:k].T @ X\n",
    "        if justQtX:\n",
    "            return QtX  # for efficient norm computations\n",
    "        # if QtX.ndim == 0:\n",
    "        #     return QtX*self.Q[:,:k] # issues with scalar\n",
    "        # else:\n",
    "        return self.Q[:,:k]@( QtX )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset, make error metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "A    = rID.load_JHTDB_data(which_component=\"x\",nsample=64,data_name=\"channel\")\n",
    "nrmX = sli.norm(A) # Frobenius norm\n",
    "relError = lambda S : np.sqrt( 1 - ( sli.norm( S.project(A,justQtX=True) )/nrmX )**2 )\n",
    "\n",
    "# Another equivalent (slower) way to compute error:\n",
    "# C, residues, rnk, singA = sli.lstsq( S.columns, A)\n",
    "# print( np.sqrt(np.sum(residues))/nrmX ) # same as sli.norm( S.columns@C - A )/nrmX"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm #3 from the Bhaskara et al. paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rID_BhaskaraAlgo3(A, k=10, xi=0.05, rng=default_rng(), SA = None):\n",
    "    \"\"\"\n",
    "    Description: Randomized ID using residual based CSS, solve least square problem to get coefficient of the new columns\n",
    "    Follows Algorithm 3 of Bhaskara et al. \"Residual Based Sampling for Online Low Rank Approximation\" (FOCS, 2019)\n",
    "\n",
    "    A is a m x n matrix, where we \"stream\" over the columns\n",
    "    k is the number of columns to save\n",
    "    xi is an target value of the Frobenius norm error (*absolute* error, not relative)\n",
    "\n",
    "    Optional: SA is a l x n matrix, a sketched version of A, which we do assume we can store in memory\n",
    "      (For now, we don't use that... later, when we start making *coefficients*, we will use that)\n",
    "\n",
    "    March 2023\n",
    "    \"\"\"\n",
    "    m, n = np.shape(A)\n",
    "    sigma = 0.\n",
    "    S     = stored_columns(k)\n",
    "\n",
    "    for column_index in range(n):\n",
    "        u = A[:,column_index]\n",
    "\n",
    "        u_perp  = u - S.project(u)\n",
    "        pu = (k/160./xi) * sli.norm(u_perp)**2\n",
    "            \n",
    "        prob_sample = np.minimum(pu, 1)\n",
    "        roll = rng.random()\n",
    "        if roll <= prob_sample: \n",
    "            S.addColumn( u, column_index )\n",
    "        if pu < 1:\n",
    "            sigma += pu\n",
    "        if pu >= 1 or sigma >= 1:\n",
    "            sigma = 0\n",
    "            S.mergePreviousWithCurrent()\n",
    "    S.mergePreviousWithCurrent()\n",
    "    return S"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run their algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096 1000\n",
      "[0, 58, 123, 182, 276, 292, 322, 341, 405, 468]\n",
      "Relative Frobenius norm error is 1.71%\n"
     ]
    }
   ],
   "source": [
    "m,n = A.shape\n",
    "print(m,n)\n",
    "k = 10\n",
    "S = rID_BhaskaraAlgo3(A, k=k, xi=10)\n",
    "# S.columns\n",
    "print( S.indices )\n",
    "\n",
    "# Find optimal error (would require a second pass through data)\n",
    "print(f\"Relative Frobenius norm error is {100*relError(S):.2f}%\" )\n",
    "# S.Q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check: choose columns at random\n",
    "Is this better, worse, or about the same as the fancy algorithm?\n",
    "\n",
    "Conclusion: about the same, or even slightly better\n",
    "\n",
    "(re-run this several times, since the results are random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[977, 259, 844, 358, 263, 62, 53, 319, 430, 334]\n",
      "Relative Frobenius norm error is 1.76%\n"
     ]
    }
   ],
   "source": [
    "rng=default_rng()\n",
    "randInd = rng.choice( n, k)\n",
    "S_rand     = stored_columns(k)\n",
    "for i in randInd:\n",
    "    S_rand.addColumn( A[:,i], i )\n",
    "S_rand.mergePreviousWithCurrent()\n",
    "\n",
    "# S.columns\n",
    "print( S_rand.indices )\n",
    "\n",
    "# Find optimal error (would require a second pass through data)\n",
    "print(f\"Relative Frobenius norm error is {100*relError(S_rand):.2f}%\" )\n",
    "# S.Q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
